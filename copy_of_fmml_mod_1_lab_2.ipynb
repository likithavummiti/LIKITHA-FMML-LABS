{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN7eBQuV3QfKyhCufX43tde",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/likithavummiti/LIKITHA-FMML-LABS/blob/main/copy_of_fmml_mod_1_lab_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "# set randomseed\n",
        "rng = np.random.default_rng(seed=42)"
      ],
      "metadata": {
        "id": "LBcstAdl608x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " dataset =  datasets.fetch_california_housing()\n",
        " # print(dataset.DESCR)  # uncomment this if you want to know more about this dataset\n",
        " # print(dataset.keys())  # if you want to know what else is there in this dataset\n",
        " dataset.target = dataset.target.astype(np.int) # so that we can classify\n",
        " print(dataset.data.shape)\n",
        " print(dataset.target.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLTG_Rtp8e-M",
        "outputId": "d31556cf-3851-46da-e31d-a45f9fdf9f67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20640, 8)\n",
            "(20640,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-60ae2e9a125e>:4: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  dataset.target = dataset.target.astype(np.int) # so that we can classify\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def NN1(traindata, trainlabel, query):\n",
        "  diff  = traindata - query  # find the difference between features. Numpy automatically takes care of the size here\n",
        "  sq = diff*diff # square the differences\n",
        "  dist = sq.sum(1) # add up the squares\n",
        "  label = trainlabel[np.argmin(dist)] # our predicted label is the label of the training data which has the least distance from the query\n",
        "  return label\n",
        "\n",
        "def NN(traindata, trainlabel, testdata):\n",
        "  # we will run nearest neighbour for each sample in the test data\n",
        "  # and collect the predicted classes in an array using list comprehension\n",
        "  predlabel = np.array([NN1(traindata, trainlabel, i) for i in testdata])\n",
        "  return predlabel"
      ],
      "metadata": {
        "id": "iK3CoTBQ8jNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def RandomClassifier(traindata, trainlabel, testdata):\n",
        "  # in reality, we don't need these arguments\n",
        "\n",
        "  classes = np.unique(trainlabel)\n",
        "  rints = rng.integers(low=0, high=len(classes), size=len(testdata))\n",
        "  predlabel = classes[rints]\n",
        "  return predlabel"
      ],
      "metadata": {
        "id": "a8wFERix8lww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Accuracy(gtlabel, predlabel):\n",
        "  assert len(gtlabel)==len(predlabel), \"Length of the groundtruth labels and predicted labels should be the same\"\n",
        "  correct = (gtlabel==predlabel).sum() # count the number of times the groundtruth label is equal to the predicted label.\n",
        "  return correct/len(gtlabel)"
      ],
      "metadata": {
        "id": "G2bo9uMN8oBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split(data, label, percent):\n",
        "  # generate a random number for each sample\n",
        "  rnd = rng.random(len(label))\n",
        "  split1 = rnd<percent\n",
        "  split2 = rnd>=percent\n",
        "  split1data = data[split1,:]\n",
        "  split1label = label[split1]\n",
        "  split2data = data[split2,:]\n",
        "  split2label = label[split2]\n",
        "  return split1data, split1label, split2data, split2label"
      ],
      "metadata": {
        "id": "I_TGDVsP8pqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testdata, testlabel, alltraindata, alltrainlabel = split(dataset.data, dataset.target, 20/100)\n",
        "print('Number of test samples = ', len(testlabel))\n",
        "print('Number of other samples = ', len(alltrainlabel))\n",
        "print('Percent of test data = ', len(testlabel)*100/len(dataset.target),'%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouOA14Fy8rla",
        "outputId": "7b1f5cfd-2235-4aad-f894-bef97cacb9f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of test samples =  4144\n",
            "Number of other samples =  16496\n",
            "Percent of test data =  20.07751937984496 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(alltraindata, alltrainlabel, 75/100)"
      ],
      "metadata": {
        "id": "HE9kHMlS8tEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainpred = NN(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Train accuracy using nearest neighbour is \", trainAccuracy)\n",
        "\n",
        "trainpred = RandomClassifier(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Train accuracy using random classifier is \", trainAccuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjNJr6HD8v7h",
        "outputId": "788a38af-11b4-4ef0-9017-62b2f338f7e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy using nearest neighbour is  1.0\n",
            "Train accuracy using random classifier is  0.164375808538163\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using nearest neighbour is \", valAccuracy)\n",
        "\n",
        "valpred = RandomClassifier(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using random classifier is \", valAccuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjJZBiWF8xcu",
        "outputId": "03dd9acc-a7cb-4aa8-c950-fa69edf04617"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy using nearest neighbour is  0.34108527131782945\n",
            "Validation accuracy using random classifier is  0.1688468992248062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(alltraindata, alltrainlabel, 75/100)\n",
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy of nearest neighbour is \", valAccuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLwkViz_8z1X",
        "outputId": "0bacb32f-399f-4e87-f9df-554a5de93f30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy of nearest neighbour is  0.34048257372654156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "testAccuracy = Accuracy(testlabel, testpred)\n",
        "print('Test accuracy is ', testAccuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxtLLlRO84Ry",
        "outputId": "fffff913-1f66-4930-b3d8-e92dc8576aa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy is  0.34917953667953666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3dGD531K3gH"
      },
      "source": [
        "### Try it out for yourself and answer:\n",
        "1. How is the accuracy of the validation set affected if we increase the percentage of validation set? What happens when we reduce it?\n",
        "2. How does the size of the train and validation set affect how well we can predict the accuracy on the test set using the validation set?\n",
        "3. What do you think is a good percentage to reserve for the validation set so that thest two factors are balanced?\n",
        "\n",
        "Answer for both nearest neighbour and random classifier. You can note down the values for your experiments and plot a graph using  <a href=https://matplotlib.org/stable/gallery/lines_bars_and_markers/step_demo.html#sphx-glr-gallery-lines-bars-and-markers-step-demo-py>plt.plot<href>. Check also for extreme values for splits, like 99.9% or 0.1%"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4btFFmgq9HLq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnYvkAZLQY7h"
      },
      "source": [
        "## Multiple Splits\n",
        "\n",
        "One way to get more accurate estimates for the test accuracy is by using <b>crossvalidation</b>. Here, we will try a simple version, where we do multiple train/val splits and take the average of validation accuracies as the test accuracy estimation. Here is a function for doing this. Note that this function will take a long time to execute."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# you can use this function for random classifier also\n",
        "def AverageAccuracy(alldata, alllabel, splitpercent, iterations, classifier=NN):\n",
        "  accuracy = 0\n",
        "  for ii in range(iterations):\n",
        "    traindata, trainlabel, valdata, vallabel = split(alldata, alllabel, splitpercent)\n",
        "    valpred = classifier(traindata, trainlabel, valdata)\n",
        "    accuracy += Accuracy(vallabel, valpred)\n",
        "  return accuracy/iterations # average of all accuracies"
      ],
      "metadata": {
        "id": "OGLatxRB9I8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Average validation accuracy is ', AverageAccuracy(alltraindata, alltrainlabel, 75/100, 10, classifier=NN))\n",
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "print('test accuracy is ',Accuracy(testlabel, testpred) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuCW-yhU9KcH",
        "outputId": "7cfa4658-7a39-44a8-c6a4-01647edbace8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average validation accuracy is  0.33584635395170215\n",
            "test accuracy is  0.34917953667953666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "go63_r-99Rvg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33GIn4x5VH-d"
      },
      "source": [
        "This is a very simple way of doing cross-validation. There are many well-known algorithms for cross-validation, like k-fold cross-validation, leave-one-out etc. This will be covered in detail in a later module. For more information about cross-validation, check <a href=https://en.wikipedia.org/wiki/Cross-validation_(statistics)>Cross-validatioin (Wikipedia)</a>\n",
        "\n",
        "### Questions\n",
        "1. Does averaging the validation accuracy across multiple splits give more consistent results?\n",
        "2. Does it give more accurate estimate of test accuracy?\n",
        "3. What is the effect of the number of iterations on the estimate? Do we get a better estimate with higher iterations?\n",
        "4. Consider the results you got for the previous questions. Can we deal with a very small train dataset or validation dataset by increasing the iterations?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sv-IZp799awK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1A  \n",
        "Yes, averaging validation accuracy across multiple splits of your dataset can often give you more consistent and reliable results compared to relying on a single validation split. This practice is commonly known as \"cross-validation.\" Cross-validation is particularly useful in machine learning and data analysis when you want to assess the performance of a model and reduce the impact of the randomness associated with a single train-test split.\n",
        "Here's why averaging validation accuracy across multiple splits is beneficial:\n",
        "\n",
        "Reduced Variance: A single train-test split can lead to variability in the validation results because it depends on the specific data points chosen for training and testing. Averaging results over multiple splits helps mitigate this variance and provides a more stable estimate of model performance.\n",
        "\n",
        "Better Generalization Assessment: Cross-validation allows you to assess how well your model generalizes to different subsets of the data. By averaging results across multiple splits, you get a more comprehensive view of your model's ability to handle various data configurations.Enhanced Hyperparameter Tuning: When tuning hyperparameters, you can use cross-validation to test different parameter settings across multiple data splits. Averaging the results helps you select the settings that perform consistently well across various data subsets.\n",
        "\n",
        "Common cross-validation techniques include k-fold cross-validation and stratified k-fold cross-validation:\n",
        "\n",
        "K-Fold Cross-Validation: In k-fold cross-validation, the dataset is divided into k equally sized subsets (folds). The model is trained and tested k times, each time using a different fold as the test set and the remaining k-1 folds as the training set. The validation scores from each iteration are averaged to obtain the final performance estimate.Stratified K-Fold Cross-Validation: Stratified k-fold cross-validation ensures that each fold maintains the same class distribution as the original dataset. This is particularly useful when dealing with imbalanced datasets.\n",
        "\n",
        "Leave-One-Out Cross-Validation (LOOCV): This is a special case of k-fold cross-validation where k is set to the number of samples in the dataset. It provides a more exhaustive assessment of model performance but can be computationally expensive for large datasets.\n"
      ],
      "metadata": {
        "id": "kl963qkkpVv3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cnzfLVZM9ed6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2A   To provide a more accurate estimate of test accuracy, it's important to use proper evaluation techniques when assessing the performance of a machine learning model. Here are some key considerations:\n",
        "\n",
        "Cross-Validation: Cross-validation is a technique where the dataset is split into multiple subsets (folds), and the model is trained and evaluated multiple times, using different subsets for testing each time. This helps in obtaining a more reliable estimate of the model's performance, reducing the impact of data variability.\n",
        "\n",
        "Holdout Testing: Keep a separate portion of your dataset as a holdout test set that you don't use during model training. After training your model on the training data, evaluate its performance on this holdout test set to get an estimate of how well it generalizes to unseen data.\n",
        "\n",
        "Metrics: Choose appropriate evaluation metrics based on the nature of your problem. Common metrics for classification tasks include accuracy, precision, recall, F1-score, and ROC AUC. For regression tasks, metrics like Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) are commonly used.\n",
        "\n",
        "Stratified Sampling: In classification tasks, ensure that your data is stratified when splitting into training and testing sets. This ensures that each class is represented proportionally in both the training and testing sets, reducing the risk of biased evaluations.\n",
        "\n",
        "Cross-Validation Strategies: Depending on your dataset size and characteristics, choose an appropriate cross-validation strategy. Common methods include k-fold cross-validation, stratified k-fold cross-validation, and leave-one-out cross-validation.\n",
        "\n",
        "Hyperparameter Tuning: Optimize the hyperparameters of your machine learning model using techniques like grid search or random search. This can lead to improvements in model performance.\n",
        "\n",
        "Outliers and Data Preprocessing: Carefully handle outliers and preprocess your data to ensure that it meets the assumptions of your chosen machine learning algorithm.\n",
        "\n",
        "Ensemble Methods: Consider using ensemble methods like bagging (e.g., Random Forests) or boosting (e.g., Gradient Boosting) to improve model performance and reduce overfitting.\n",
        "\n",
        "In summary, obtaining a more accurate estimate of test accuracy requires a combination of proper data splitting, evaluation metrics, preprocessing, and model tuning. It's essential to follow best practices in machine learning to ensure that your model's performance assessment is as accurate as possible and that it generalizes well to unseen data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aF58koOSq15S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "orXkRmaR9g-e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3A   The effect of the number of iterations on an estimate can vary depending on the specific estimation method and the problem being solved. In general, increasing the number of iterations can lead to a more accurate estimate, but there are diminishing returns, and other factors may come into play. Here are some general considerations:\n",
        "\n",
        "Convergence: In many iterative estimation algorithms, the estimate tends to approach a stable value as the number of iterations increases. This means that initially, increasing iterations can lead to a significant improvement in accuracy, but as you continue to increase iterations, the improvement becomes smaller and eventually plateaus. This behavior is often described as convergence.\n",
        "\n",
        "Computational Cost: More iterations generally require more computational resources, such as time and processing power. There may be a trade-off between the accuracy of the estimate and the computational cost. You need to balance the computational resources available with the desired level of accuracy.\n",
        "\n",
        "Precision vs. Accuracy: It's important to distinguish between precision and accuracy. Precision refers to how consistent the estimates are when repeated measurements are made under the same conditions. Accuracy, on the other hand, is how close the estimate is to the true value. Increasing iterations may improve precision (reduce variability) but not necessarily accuracy if the estimation method is biased or based on incorrect assumptions.\n",
        "\n",
        "Convergence Criteria: Many iterative algorithms use convergence criteria to stop the iterations when a certain level of accuracy is achieved. Setting appropriate convergence criteria is essential to avoid unnecessary iterations and computational overhead.\n",
        "\n",
        "Initial Guess: The choice of the initial guess or starting point can also influence the effect of iterations. A poor initial guess may require more iterations to converge to an accurate estimate.\n",
        "\n",
        "In summary, increasing the number of iterations can lead to a better estimate, but the relationship between the number of iterations and the quality of the estimate is not necessarily linear, and there are trade-offs to consider, including computational cost. It's essential to understand the specific estimation method and the problem context to determine the optimal number of iterations for your needs. In some cases, additional iterations may provide only marginal improvements in accuracy, making it more efficient to stop earlier.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TJJycuaprwf4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "v_ztNk-X9skm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4A   Increasing the number of iterations may help mitigate the limitations of a small training dataset to some extent, but it's not a complete solution to the challenges posed by a small dataset. Here are some considerations:\n",
        "\n",
        "Overfitting: If you have a very small training dataset, increasing the number of iterations may lead to overfitting. Overfitting occurs when a model learns to fit the noise in the training data, rather than the underlying patterns. This can result in poor generalization to new, unseen data. Therefore, it's crucial to strike a balance between the number of iterations and the capacity of your model. Regularization techniques and careful monitoring of validation performance can help prevent overfitting.\n",
        "\n",
        "Data Quality: The quality of the training data is critical. If your small dataset contains errors, outliers, or biases, increasing iterations won't necessarily improve the quality of the model's estimates. Data preprocessing and cleaning are essential steps to address these issues.\n",
        "\n",
        "Validation Dataset: While increasing iterations might improve the model's performance on the training data, it may not necessarily improve its performance on a small validation dataset. The validation dataset is crucial for assessing the model's ability to generalize to new data. If your validation dataset is small, it may be more prone to random variations, and the model's performance on it may not be a reliable indicator of generalization.\n",
        "\n",
        "Model Complexity: The choice of model architecture is also essential. A very complex model may have a higher risk of overfitting a small dataset, even with many iterations. You may need to consider using simpler models or techniques like transfer learning, which leverage pre-trained models on larger datasets.\n",
        "\n",
        "Data Augmentation: Data augmentation techniques can artificially increase the effective size of your dataset by creating new training examples from the existing ones. This can be particularly useful when working with small datasets.\n",
        "\n",
        "Regularization: Regularization techniques like L1 or L2 regularization can help control overfitting when dealing with small datasets.\n",
        "\n",
        "In summary, while increasing the number of iterations may help a model learn more from a small training dataset, it's just one aspect of addressing the challenges associated with limited data. To build robust and generalizable models with small datasets, you should consider a combination of techniques, including data preprocessing, data augmentation, regularization, and careful model selection, in addition to optimizing the number of iterations.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "83zqehzMrzDT"
      }
    }
  ]
}